# Disney's API Test Automation Framework

## Setup
1. Install [Python](https://www.python.org/downloads/) >= 3.11 and [Poetry](https://python-poetry.org/docs/).
2. Install PyCharm / IntelliJ, clone the project.
3. Create poetry virtual environment and install dependencies: `poetry shell` & `poetry install`.
4. Setup Python interpreter in PyCharm / IntelliJ and configure PyTest as test runner in Settings:
   - "Add New Interpreter" → "Add Local Interpreter" → "Poetry environment" → "Existing environment"
   - `[Windows]` Provide the Python path to the virtual environment:
    ```C:\Users\<USER>\AppData\Local\pypoetry\Cache\virtualenvs\ip-manager-taf-api-<HASH>-py3.11\Scripts\python.exe```
5. `.config.yaml` should be created in root of project. Use `config-template.yaml` as config file template. 
   You need to create users for facility. To create a user you need to create a role. To create a facility you need to create a customer.

## How to run tests?
Tests can run in DEV, PROD environments. You can change default running env in `.config.yaml` file that you created. Or provide `--env` flag while executing tests. It will override `CONFIG.env`.
```bash
python -m pytest tests/ -s
python -m pytest -n auto --reruns 3 -s --env=dev -v tests/  # Run all available tests.
python -m pytest -n auto --reruns 3 -s --env=dev -v -m smoke tests/  # Run all smoke tests. Tests with smoke marker.

# Flags
# `-n auto` = level of parallelism to run multiple tests simultaneously.
# `--reruns 5` = maximum number of times the tests to run if failed.
# - `--reruns-delay 1` = amount of seconds to wait before the next re-run.
# `--lf` or `--last-failed` = to run last failed tests.
# `-v` = verbose, to get more info.
# `-m` = mark expression.
```

## Configuration 
1. `.config.yaml` includes `API URLs` for DEV, STAGING and PROD envs. 
   - It includes users credentials for superuser, facility admin, facility driver, facility simple user.
   - Superuser -> can control all facilities. Superuser can create a new facility, and facility admin.
   - Facility admin -> can control all users only for his/her facility. 
   - Facility driver -> this type of user, has driver user role.
   - Facility user -> this type of user has no EDIT and VIEW permission, and assigned to autogenerated facility.
   - NOTE! Create facility admin, driver and user by yourself, via Web UI. [TMP SOLUTION]
2. If you change the content and schema of config file, you need to change following files too:
   ```text
   /core/enums/environments.py # The list of env: DEV, STAGING, PROD.
   /core/config/__init__.py # Change this file, if you change the schema of config file.
   /tests/conftest.py # Change PyTest hooks.
   /paths.py # The paths for common project files. Change here if you edit the name of config file.
   ```

## Project structure
```text
`\core` includes anything related to framework (HTTP client, API general responses validation, helpers, config).
`\api` contains anything related to product API models, endpoints, query string params.
`\tests` contains all tests.
`\data` includes test data, and fake request models.
`.config.yaml` config file.
```

## Markers in code
You can use search to find these markers.
```text
#TODO -> the tasks that need to be done in the future.
```

## How to write test for new feature?
1. Learn new feature, create test scenarios, test cases.
2. Create success response models in `api/responses`.
3. Create requests endpoints in `api/requests`. Validate schema depending on status code. 
4. Write tests in `tests` folder. Some tests require fixtures. Create fixtures inside `tests/fixtures`.

## Models
```text
Request models -> fake request models are inside test data layer.

Response -> all successful and error response models contain status, message, data, error.
SuccessResponse, checks that status is False, data should be object.
ErrorResponse, checks that status is True, error should be object.

Common responses models in common_models.py. Ex: AuthErrorResponse.
    
Other response models are created for successful responses per API.
```

## Response validation plan
1. Validate HTTP status code
2. Validate schema: convert response.json() to pydantic model to check data types, and verify that all fields are present. 
   - `status` field content is checked via pydantic `SuccessResponse` and `ErrorResponse`.
   - `error` schema is NOT validated for all responses, as it is not defined, and generated by backend framework.
   - But we validate common error responses such as authorization errors, wrong path errors.
   - **TODO: `message` is not validated separately yet for success, error. It will be done in the future, when backend will support this feature.**
3. Validate data, and assert result with expected.

## Constraints
```text
1. @mobile() # Fixture for mobile which uses `/?is_for_mobile=true`
2. @users() # Authentication -> auth, unauth requests. Users should be created automatically by superuser. 
   - How the new user is created? Superuser creates facility, new role, then new user. Then config.users is updated. Users should be created by pytest hook.
3. Endpoints, endpoints with section, request & response model.
   - Endpoints with section and without section how should be tested that we have error?
     All positive cases, should include the section.
     Web and mobile always use section param.
     So, tests without section param will be considered as NEGATIVE. And we can have separate tests and overwrite API functions. That's okay.
```
